import streamlit as st
import os

st.set_page_config(page_title="Local Agent RAG Demo", layout="centered")

st.title("Local LLM + RAG + Agent Demo")
st.caption("Runs fully local or with vector retrieval, deployed via GitHub â†’ Streamlit Cloud")

# Placeholder for local model (you will replace with your GGUF loader later)
@st.cache_resource
def load_local_model():
    # Example (replace this with your real GGUF path/loader if needed)
    # from llama_cpp import Llama
    # return Llama(model_path="./models/mistral-7b-instruct.Q4_K_M.gguf", n_ctx=2048)
    return None

llm = load_local_model()

st.subheader("Competitor Strategy Researcher Agent")

user_input = st.text_input("Enter a market research question:", "Who are the top competitors and their marketing strengths?")

if st.button("Run Research"):
    if llm is None:
        st.warning("Local model not connected yet. This is a demo response.")
        # Dummy safe demo output
        st.write("Tesla, Rivian, and Lucid lead with sustainability branding, premium UX, and community engagement.")
    else:
        # Real local inference call (simple example)
        try:
            result = llm.create_chat_completion(
                messages=[{"role": "user", "content": user_input}],
                max_tokens=80
            )
            st.write(result["choices"][0]["message"]["content"])
        except Exception as e:
            st.error(f"Local inference failed: {e}")

st.divider()
st.markdown("### GitHub Deployment Steps")
st.markdown("""
1. Create a repo in GitHub (example: **PFolio-Streamlit-Agent**)
2. Upload this file as `app.py`
3. (Optional) Add a `models/` folder and upload your GGUF model
4. Add `requirements.txt`:

